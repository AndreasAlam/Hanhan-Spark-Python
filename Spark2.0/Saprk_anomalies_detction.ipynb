{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "import operator\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\")\\\n",
    "       .appName(\"Anomalies Detection\")\\\n",
    "       .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "       .getOrCreate()\\\n",
    "\n",
    "sparkCt = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   id|         rawFeatures|\n",
      "+-----+--------------------+\n",
      "|44263|[udp, SF, -0.1585...|\n",
      "|44264|[tcp, SF, -0.1585...|\n",
      "|44265|[tcp, SF, -0.1585...|\n",
      "|44266|[tcp, SF, -0.1585...|\n",
      "|44267|[tcp, SF, -0.1585...|\n",
      "|44268|[udp, SF, -0.1585...|\n",
      "|44269|[tcp, SF, -0.1585...|\n",
      "+-----+--------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def readData(filename):\n",
    "    rawDF = spark.read.parquet(filename).cache()\n",
    "    return rawDF\n",
    "    \n",
    "file_path = \"logs-features-sample/\"\n",
    "rawDF = readData(file_path)\n",
    "rawDF.show(n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_onehot(lst, indices, unique_values, c):\n",
    "    zs = [0.0]*c\n",
    "    rest_lst = [float(lst[k]) for k in range(len(lst)) if k not in indices]\n",
    "    for pos in indices:\n",
    "        idx = unique_values.index(Row(lst[pos]))\n",
    "        zs[idx] = 1.0\n",
    "    zs.extend(rest_lst)\n",
    "    return zs\n",
    "    \n",
    "    \n",
    "# in rawFeatures, the first 2 categorical data convert to one hot vector such as [0,0,1,0,1]\n",
    "# extend the one-hot vector with original numerical list, and all convert to Double type\n",
    "# put the numerical list to a new column called \"features\"\n",
    "def cat2Num(df, indices):\n",
    "    unique_values = []\n",
    "    for i in indices:\n",
    "        d = udf(lambda r: r[i], StringType())\n",
    "        dt = df.select(d(df.rawFeatures)).distinct().collect()\n",
    "        unique_values.extend(dt)\n",
    "\n",
    "    unique_count = len(unique_values)\n",
    "    convertUDF = udf(lambda r: to_onehot(r, indices, unique_values, unique_count), ArrayType(DoubleType()))\n",
    "    newdf = df.withColumn(\"features\", convertUDF(df.rawFeatures))\n",
    "\n",
    "    return newdf\n",
    "\n",
    "\n",
    "def addScore(df):\n",
    "    cluster_dict = {}\n",
    "    clusters_list = df.select(\"prediction\").collect()\n",
    "    for c in clusters_list:\n",
    "        cluster_dict[c] = cluster_dict.setdefault(c,0.0)+1.0\n",
    "    sorted_clusters = sorted(cluster_dict.items(), key=operator.itemgetter(1))  # sort by value\n",
    "    n_max = sorted_clusters[-1][1]\n",
    "    n_min = sorted_clusters[0][1]\n",
    "    score_udf = udf(lambda p: float(n_max - cluster_dict.get(Row(p)))/(n_max - n_min), DoubleType())\n",
    "    score_df = df.withColumn(\"score\", score_udf(df.prediction))\n",
    "    return score_df\n",
    "\n",
    "\n",
    "def detect(rawDF, k, t):\n",
    "    # Encoding categorical features using one-hot.\n",
    "    df1 = cat2Num(rawDF, [0, 1]).cache()\n",
    "    df1.show(n=2, truncate=False)\n",
    "\n",
    "    # Clustering points using KMeans\n",
    "    features = df1.select(\"features\").rdd.map(lambda row: row[0]).cache()\n",
    "    model = KMeans.train(features, k, maxIterations=40, runs=10, initializationMode=\"random\", seed=410)\n",
    "\n",
    "    # Adding the prediction column to df1\n",
    "    modelBC = sparkCt.broadcast(model)\n",
    "    predictUDF = udf(lambda x: modelBC.value.predict(x), StringType())\n",
    "    df2 = df1.withColumn(\"prediction\", predictUDF(df1.features)).cache()\n",
    "    df2.show(n=3, truncate=False)\n",
    "\n",
    "    # Adding the score column to df2; The higher the score, the more likely it is an anomaly\n",
    "    df3 = self.addScore(df2).cache()\n",
    "    df3.show(n=3, truncate=False)\n",
    "\n",
    "    return df3.where(df3.score > t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|   id|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|44263|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|\n",
      "|44264|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|\n",
      "|44265|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|\n",
      "|44266|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|\n",
      "|44267|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|\n",
      "|44268|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|\n",
      "|44269|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = cat2Num(rawDF, [0, 1]).cache()\n",
    "df1.show(n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering points using KMeans\n",
    "k = 8\n",
    "features = df1.select(\"features\").rdd.map(lambda row: row[0]).cache()  # row is a list, row[0] is the feature list\n",
    "model = KMeans.train(features, k, maxIterations=40, initializationMode=\"random\", seed=410)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|   id|         rawFeatures|            features|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|44263|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|         3|\n",
      "|44264|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         5|\n",
      "|44265|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         3|\n",
      "|44266|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         5|\n",
      "|44267|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         3|\n",
      "|44268|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|         3|\n",
      "|44269|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding the prediction column to df1\n",
    "modelBC = sparkCt.broadcast(model)\n",
    "predictUDF = udf(lambda x: modelBC.value.predict(x), StringType())\n",
    "df2 = df1.withColumn(\"prediction\", predictUDF(df1.features)).cache()\n",
    "df2.show(n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+-------------------+\n",
      "|   id|         rawFeatures|            features|prediction|              score|\n",
      "+-----+--------------------+--------------------+----------+-------------------+\n",
      "|44263|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|         3|                0.0|\n",
      "|44264|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         5|0.02132185100643287|\n",
      "|44265|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         3|                0.0|\n",
      "|44266|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         5|0.02132185100643287|\n",
      "|44267|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         3|                0.0|\n",
      "|44268|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|         3|                0.0|\n",
      "|44269|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7| 0.8150031126789791|\n",
      "+-----+--------------------+--------------------+----------+-------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding the score column to df2; The higher the score, the more likely it is an anomaly\n",
    "# higher the score, means the member count of that cluster is less\n",
    "df3 = addScore(df2).cache()\n",
    "df3.show(n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+------------------+\n",
      "|   id|         rawFeatures|            features|prediction|             score|\n",
      "+-----+--------------------+--------------------+----------+------------------+\n",
      "|44269|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|0.8150031126789791|\n",
      "|44281|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|0.8150031126789791|\n",
      "|44286|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|         6|0.9452946669433493|\n",
      "|44290|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         6|0.9452946669433493|\n",
      "|44302|[tcp, REJ, -0.158...|[1.0, 0.0, 0.0, 0...|         1|0.8564795600747043|\n",
      "|44304|[tcp, REJ, -0.158...|[1.0, 0.0, 0.0, 0...|         1|0.8564795600747043|\n",
      "|44305|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|0.8150031126789791|\n",
      "+-----+--------------------+--------------------+----------+------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = 0.79\n",
    "df3.where(df3.score > t).show(n=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
