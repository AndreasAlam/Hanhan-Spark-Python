{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark - How to Use Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing spark session\n",
    "sc = SparkContext(\"local[2]\", appName=\"spark streaming twitter sentiment\")  # local n means n threads can be used\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0| @user when a fat...|\n",
      "|  2|    0|@user @user thank...|\n",
      "|  3|    0|  bihday your maj...|\n",
      "|  4|    0|#model   i love u...|\n",
      "|  5|    0| factsguide: soci...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('twitter_sentiment.csv', header=True)\n",
    "df = df.withColumn(\"label\", df[\"label\"].cast(tp.IntegerType()))\n",
    "df = df.withColumn(\"id\", df[\"id\"].cast(tp.IntegerType()))\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31962\n",
      "31000\n",
      "962\n",
      "[Row(min(id)=31001)] [Row(max(id)=31962)]\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "\n",
    "training_df = df.limit(31000)\n",
    "print(training_df.count())\n",
    "\n",
    "testing_df = df.filter(df['id'] > 31000)\n",
    "print(testing_df.count())\n",
    "print(testing_df.groupBy().min('id').collect(), testing_df.groupBy().max('id').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Pipeline, Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 100\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)\n",
    "# define stage 4: Logistic Regression Model\n",
    "model_lg = LogisticRegression(featuresCol= 'vector', labelCol= 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model_lg])\n",
    "\n",
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|               tweet|prediction|label|\n",
      "+--------------------+----------+-----+\n",
      "|the excitement of...|       0.0|    0|\n",
      "|as a huge @user f...|       0.0|    0|\n",
      "|immoality   #wave...|       0.0|    0|\n",
      "|#repost from @use...|       0.0|    0|\n",
      "| @user nice discu...|       0.0|    0|\n",
      "|love them !! #che...|       0.0|    0|\n",
      "|this so of shit i...|       0.0|    0|\n",
      "|#travel#girl#russ...|       0.0|    0|\n",
      "| @user when a for...|       0.0|    0|\n",
      "|first day as a 26...|       0.0|    0|\n",
      "|well, @user is fo...|       0.0|    0|\n",
      "|my first comic bo...|       0.0|    0|\n",
      "|spray painting ad...|       0.0|    0|\n",
      "|can #lighttherapy...|       0.0|    0|\n",
      "|.@user fyi since ...|       0.0|    0|\n",
      "|it's really happy...|       0.0|    0|\n",
      "|even when i don't...|       0.0|    0|\n",
      "|@user stands watc...|       0.0|    0|\n",
      "|friday feeling   ...|       0.0|    0|\n",
      "| @user just 1 mor...|       0.0|    0|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = pipelineFit.transform(testing_df).select('tweet','prediction', 'label')\n",
    "\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "It's desinitely looks simple and organized to use machine learning pipeline, but there are some limitations:\n",
    "* It has to process Spark DataFrame\n",
    "* When I was trying to apply this pipeline in spark streaming data, it always gave me errors in data schema related issue. Since streaming data is using rdd, you have to convert to dataframe, where schema s required, but there could be different unintelligent schema issues appear.\n",
    "\n",
    "## Reference\n",
    "* https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda3_env",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
