{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "import operator\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"streaming anomalies detection\") # run locally with 2 cores\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 1)  # 1 second per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------+\n",
      "|   id|                                                                                        rawFeatures|\n",
      "+-----+---------------------------------------------------------------------------------------------------+\n",
      "|44263|[udp, SF, -0.158545578037, -0.0314405270803, -0.111401325544, 0.0, -0.00317668482933, -0.0031766...|\n",
      "|44264|[tcp, SF, -0.158545578037, 0.197720022657, -0.111401325544, 0.0, -0.00317668482933, -0.003176684...|\n",
      "|44265|[tcp, SF, -0.158545578037, -0.0246212390762, 0.0353165052995, 0.0, -0.00317668482933, -0.0031766...|\n",
      "|44266|[tcp, SF, -0.158545578037, 0.025711600954, -0.0995500693817, 0.0, -0.00317668482933, -0.00317668...|\n",
      "|44267|[tcp, SF, -0.158545578037, -0.0263422974773, 1.01349541435, 0.0, -0.00317668482933, -0.003176684...|\n",
      "|44268|[udp, SF, -0.158545578037, -0.0334213678815, -0.108555583183, 0.0, -0.00317668482933, -0.0031766...|\n",
      "|44269|[tcp, SF, -0.158545578037, -0.0289401214788, 0.391394520984, 0.0, -0.00317668482933, -0.00317668...|\n",
      "+-----+---------------------------------------------------------------------------------------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def readData(filename):\n",
    "    rawDF = sqlContext.read.parquet(filename).cache()\n",
    "    return rawDF\n",
    "    \n",
    "file_path = \"logs-features-sample/\"\n",
    "rawDF = readData(file_path)\n",
    "rawDF.show(n=7, truncate=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_onehot(lst, indices, unique_values, c):\n",
    "    zs = [0.0]*c\n",
    "    rest_lst = [float(lst[k]) for k in range(len(lst)) if k not in indices]\n",
    "    for pos in indices:\n",
    "        idx = unique_values.index(Row(lst[pos]))\n",
    "        zs[idx] = 1.0\n",
    "    zs.extend(rest_lst)\n",
    "    return zs\n",
    "    \n",
    "    \n",
    "# in rawFeatures, the first 2 categorical data convert to one hot vector such as [0,0,1,0,1]\n",
    "# extend the one-hot vector with original numerical list, and all convert to Double type\n",
    "# put the numerical list to a new column called \"features\"\n",
    "def cat2Num(df, indices):\n",
    "    unique_values = []\n",
    "    for i in indices:\n",
    "        d = udf(lambda r: r[i], StringType())\n",
    "        dt = df.select(d(df.rawFeatures)).distinct().collect()\n",
    "        unique_values.extend(dt)\n",
    "\n",
    "    unique_count = len(unique_values)\n",
    "    convertUDF = udf(lambda r: to_onehot(r, indices, unique_values, unique_count), ArrayType(DoubleType()))\n",
    "    newdf = df.withColumn(\"features\", convertUDF(df.rawFeatures))\n",
    "\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------+--------------------------------------------------+\n",
      "|   id|                                       rawFeatures|                                          features|\n",
      "+-----+--------------------------------------------------+--------------------------------------------------+\n",
      "|44263|[udp, SF, -0.158545578037, -0.0314405270803, -0...|[0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0...|\n",
      "|44264|[tcp, SF, -0.158545578037, 0.197720022657, -0.1...|[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0...|\n",
      "|44265|[tcp, SF, -0.158545578037, -0.0246212390762, 0....|[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0...|\n",
      "|44266|[tcp, SF, -0.158545578037, 0.025711600954, -0.0...|[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0...|\n",
      "|44267|[tcp, SF, -0.158545578037, -0.0263422974773, 1....|[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0...|\n",
      "|44268|[udp, SF, -0.158545578037, -0.0334213678815, -0...|[0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0...|\n",
      "|44269|[tcp, SF, -0.158545578037, -0.0289401214788, 0....|[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0...|\n",
      "+-----+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = cat2Num(rawDF, [0, 1]).cache()\n",
    "df1.show(n=7, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99095\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print df1.count()  # number of rows\n",
    "print len(df1.select(\"features\").first()[0])  # dimention is 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99094 0\n"
     ]
    }
   ],
   "source": [
    "max_value = df1.agg({\"id\": \"max\"}).collect()[0][0]\n",
    "min_value = df1.agg({\"id\": \"min\"}).collect()[0][0]\n",
    "print max_value, min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_features = df1.where(df1.id >= 90000).select(\"features\").rdd.map(lambda row: Vectors.dense(row[0]))\n",
    "testing_features = df1.where(df1.id < 90000).select(\"features\").rdd.map(lambda row: Vectors.dense(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingQueue = [training_features]\n",
    "testingQueue = [testing_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingStream = ssc.queueStream(trainingQueue)\n",
    "testingStream = ssc.queueStream(testingQueue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create a model with random clusters and specify the number of clusters to find\n",
    "dimension = 50\n",
    "model = StreamingKMeans(k=8, decayFactor=1.0).setRandomCenters(dimension, 1.0, 410)\n",
    "\n",
    "# Now register the streams for training\n",
    "model.trainOn(trainingStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = model.predictOn(testingStream)\n",
    "result.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-01-28 22:02:32\n",
      "-------------------------------------------\n",
      "4\n",
      "7\n",
      "2\n",
      "7\n",
      "2\n",
      "7\n",
      "7\n",
      "2\n",
      "2\n",
      "2\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-01-28 22:02:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()  # start streaming\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
