{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing spark session\n",
    "sc = SparkContext(\"local[2]\", appName=\"spark streaming twitter sentiment\")  # local n means n threads can be used\n",
    "ssc = StreamingContext(sc, 1)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0| @user when a fat...|\n",
      "|  2|    0|@user @user thank...|\n",
      "|  3|    0|  bihday your maj...|\n",
      "|  4|    0|#model   i love u...|\n",
      "|  5|    0| factsguide: soci...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('twitter_sentiment.csv', header=True)\n",
    "df = df.withColumn(\"label\", df[\"label\"].cast(tp.IntegerType()))\n",
    "df = df.withColumn(\"id\", df[\"id\"].cast(tp.IntegerType()))\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31962\n",
      "31000\n",
      "962\n",
      "[Row(min(id)=31001)] [Row(max(id)=31962)]\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "\n",
    "training_df = df.limit(31000)\n",
    "print(training_df.count())\n",
    "\n",
    "testing_df = df.filter(df['id'] > 31000)\n",
    "print(testing_df.count())\n",
    "print(testing_df.groupBy().min('id').collect(), testing_df.groupBy().max('id').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 100\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)\n",
    "# define stage 4: Logistic Regression Model\n",
    "model_lg = LogisticRegression(featuresCol= 'vector', labelCol= 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model_lg])\n",
    "\n",
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(tweet_text):\n",
    "\ttry:\n",
    "    # create a spark dataframe\n",
    "\t\twordsDataFrame = spark.createDataFrame(tweet_text)\n",
    "    # transform the data using the pipeline and get the predicted sentiment\n",
    "\t\tpipelineFit.transform(wordsDataFrame).select('tweet','prediction').show()\n",
    "\texcept : \n",
    "\t\tprint('No data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:09\n",
      "-------------------------------------------\n",
      "Output 0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:09\n",
      "-------------------------------------------\n",
      "Output ['user', 'when', 'a', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'he', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction', 'run']\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:10\n",
      "-------------------------------------------\n",
      "Output 0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:10\n",
      "-------------------------------------------\n",
      "Output ['user', 'user', 'thanks', 'for', 'lyft', 'credit', 'i', 'can', 't', 'use', 'cause', 'they', 'don', 't', 'offer', 'wheelchair', 'vans', 'in', 'pdx', 'disapointed', 'getthanked']\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:11\n",
      "-------------------------------------------\n",
      "Output 0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:11\n",
      "-------------------------------------------\n",
      "Output ['bihday', 'your', 'majesty']\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:12\n",
      "-------------------------------------------\n",
      "Output 0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:12\n",
      "-------------------------------------------\n",
      "Output ['model', 'i', 'love', 'u', 'take', 'with', 'u', 'all', 'the', 'time', 'in', 'ur']\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:13\n",
      "-------------------------------------------\n",
      "Output 0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:13\n",
      "-------------------------------------------\n",
      "Output ['factsguide', 'society', 'now', 'motivation']\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-25 10:06:14\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "test_df = spark.read.csv('sample.csv', header=True)\n",
    "\n",
    "rddQueue = []\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"tweet\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "regexTokenized = regexTokenizer.transform(test_df)\n",
    "\n",
    "for r in regexTokenized.rdd.collect():\n",
    "    rddQueue += [sc.parallelize([r['label']+str(r['words'])])]  # parallelize() to make rdd distributable\n",
    "    \n",
    "inputStream = ssc.queueStream(rddQueue)\n",
    "inputStream.map(lambda x: \"Output \" + x[0]).pprint()\n",
    "inputStream.map(lambda x: \"Output \" + x[1:]).pprint()\n",
    "\n",
    "ssc.start()\n",
    "sleep(4)  # the time decides when the program will stop, stop earlier, all the data may not be processed\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda3_env",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
