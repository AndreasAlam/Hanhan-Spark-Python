{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing spark session\n",
    "sc = SparkContext(\"local[2]\", appName=\"spark streaming twitter sentiment\")  # local n means n threads can be used\n",
    "ssc = StreamingContext(sc, 1)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0| @user when a fat...|\n",
      "|  2|    0|@user @user thank...|\n",
      "|  3|    0|  bihday your maj...|\n",
      "|  4|    0|#model   i love u...|\n",
      "|  5|    0| factsguide: soci...|\n",
      "+---+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = tp.StructType([tp.StructField('id', tp.StringType(), True),\n",
    "         tp.StructField('label', tp.StringType(), True),\n",
    "         tp.StructField('tweet', tp.StringType(), True)])\n",
    "\n",
    "sample_df = spark.read.schema(schema).csv('sample.csv', header=True)\n",
    "\n",
    "sample_df.show()\n",
    "sample_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Sentiment Analysis\n",
    "\n",
    "I'm in my hometown at this moment, the network is too bad and most websites are not reachable. So the searchable resources are very limited. Meanwhile, spark machine learning has many limitations when you want to do sentmengt analysis yourself with machine learning algorithms. Therefore, here I'm using the most basic pre-trained sentiment analysis tool to show how spark streaming works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-01-05 17:15:38\n",
      "-------------------------------------------\n",
      "Predicted Results: {'neg': 0.385, 'neu': 0.615, 'pos': 0.0, 'compound': -0.8296},  Label: 0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-01-05 17:15:39\n",
      "-------------------------------------------\n",
      "Predicted Results: {'neg': 0.0, 'neu': 0.744, 'pos': 0.256, 'compound': 0.6705},  Label: 0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-01-05 17:15:40\n",
      "-------------------------------------------\n",
      "Predicted Results: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},  Label: 0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-01-05 17:15:41\n",
      "-------------------------------------------\n",
      "Predicted Results: {'neg': 0.0, 'neu': 0.663, 'pos': 0.337, 'compound': 0.7249},  Label: 0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-01-05 17:15:42\n",
      "-------------------------------------------\n",
      "Predicted Results: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},  Label: 0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-01-05 17:15:43\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "rddQueue = []\n",
    "\n",
    "for r in sample_df.rdd.collect():\n",
    "    rddQueue += [sc.parallelize([r['label'] + r['tweet']])]  # parallelize() to make rdd distributable\n",
    "    \n",
    "inputStream = ssc.queueStream(rddQueue)\n",
    "inputStream.map(lambda x: \"Predicted Results: \" + str(sia.polarity_scores(x[1:])) + \",  Label: \" + x[0]).pprint()\n",
    "\n",
    "ssc.start()\n",
    "sleep(4)  # the time decides when the program will stop, stop earlier, all the data may not be processed\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda3_env",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
